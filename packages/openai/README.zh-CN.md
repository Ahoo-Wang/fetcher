# @ahoo-wang/fetcher-openai

[![npm version](https://img.shields.io/npm/v/@ahoo-wang/fetcher-openai.svg)](https://www.npmjs.com/package/@ahoo-wang/fetcher-openai)
[![Build Status](https://github.com/Ahoo-Wang/fetcher/actions/workflows/ci.yml/badge.svg)](https://github.com/Ahoo-Wang/fetcher/actions)
[![codecov](https://codecov.io/gh/Ahoo-Wang/fetcher/graph/badge.svg?token=JGiWZ52CvJ)](https://codecov.io/gh/Ahoo-Wang/fetcher)
[![License](https://img.shields.io/npm/l/@ahoo-wang/fetcher.svg)](https://github.com/Ahoo-Wang/fetcher/blob/main/LICENSE)
[![npm downloads](https://img.shields.io/npm/dm/@ahoo-wang/fetcher-openai.svg)](https://www.npmjs.com/package/@ahoo-wang/fetcher-openai)
[![npm bundle size](https://img.shields.io/bundlephobia/minzip/%40ahoo-wang%2Ffetcher-openai)](https://www.npmjs.com/package/@ahoo-wang/fetcher-openai)
[![Node Version](https://img.shields.io/node/v/@ahoo-wang/fetcher-openai.svg)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Ahoo-Wang/fetcher)
[![Storybook](https://img.shields.io/badge/Storybook-äº¤äº’å¼æ–‡æ¡£-FF4785)](https://fetcher.ahoo.me/?path=/docs/openai-introduction--docs)

> ğŸš€ **ç°ä»£åŒ– â€¢ ç±»å‹å®‰å…¨ â€¢ æµå¼å°±ç»ª** - åŸºäº Fetcher ç”Ÿæ€æ„å»ºçš„å®Œæ•´ OpenAI å®¢æˆ·ç«¯

åŸºäº Fetcher ç”Ÿæ€æ„å»ºçš„ç°ä»£åŒ–ã€ç±»å‹å®‰å…¨çš„ OpenAI å®¢æˆ·ç«¯åº“ã€‚æä¾›ä¸ OpenAI Chat Completions API çš„æ— ç¼é›†æˆï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”ï¼Œå¹¶å…·æœ‰å®Œæ•´çš„ TypeScript æ”¯æŒå’Œè‡ªåŠ¨è¯·æ±‚å¤„ç†ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **å®Œæ•´ TypeScript æ”¯æŒ**: ä¸¥æ ¼ç±»å‹æ£€æŸ¥å’Œ IntelliSense æ”¯æŒ
- ğŸ“¡ **åŸç”Ÿæµå¼æ”¯æŒ**: å†…ç½®æœåŠ¡å™¨å‘é€äº‹ä»¶æµæ”¯æŒï¼Œè‡ªåŠ¨ç»ˆæ­¢æ£€æµ‹
- ğŸ¯ **å£°æ˜å¼ API**: ä½¿ç”¨è£…é¥°å™¨æ¨¡å¼å®ç°ç®€æ´ã€å¯è¯»çš„ä»£ç 
- ğŸ”§ **Fetcher ç”Ÿæ€é›†æˆ**: åŸºäºå¼ºå¤§çš„ Fetcher HTTP å®¢æˆ·ç«¯æ„å»ºï¼Œæ”¯æŒé«˜çº§åŠŸèƒ½
- ğŸ“¦ **ä¼˜åŒ–åŒ…ä½“ç§¯**: å®Œæ•´çš„æ ‘æ‘‡æ”¯æŒï¼Œæœ€å°åŒ–åŒ…ä½“ç§¯
- ğŸ§ª **å…¨é¢æµ‹è¯•**: ä½¿ç”¨ Vitest è¿›è¡Œ 100% æµ‹è¯•è¦†ç›–
- ğŸ”„ **æ¡ä»¶ç±»å‹**: åŸºäºæµå¼é…ç½®çš„æ™ºèƒ½è¿”å›ç±»å‹
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**: å¼ºå¤§çš„é”™è¯¯å¤„ç†å’Œè¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯
- âš¡ **é«˜æ€§èƒ½**: ä¸ºå¼€å‘å’Œç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
- ğŸ”Œ **å¯æ‰©å±•**: æ˜“äºé›†æˆè‡ªå®šä¹‰æ‹¦æˆªå™¨å’Œä¸­é—´ä»¶

## ğŸ“¦ å®‰è£…

### å…ˆå†³æ¡ä»¶

- **Node.js**: >= 16.0.0
- **TypeScript**: >= 5.0 (æ¨è)

### ä½¿ç”¨ npm å®‰è£…

```bash
npm install @ahoo-wang/fetcher-openai @ahoo-wang/fetcher @ahoo-wang/fetcher-decorator @ahoo-wang/fetcher-eventstream
```

### ä½¿ç”¨ yarn å®‰è£…

```bash
yarn add @ahoo-wang/fetcher-openai @ahoo-wang/fetcher @ahoo-wang/fetcher-decorator @ahoo-wang/fetcher-eventstream
```

### ä½¿ç”¨ pnpm å®‰è£…

```bash
pnpm add @ahoo-wang/fetcher-openai @ahoo-wang/fetcher @ahoo-wang/fetcher-decorator @ahoo-wang/fetcher-eventstream
```

### å¯¹ç­‰ä¾èµ–

æ­¤åŒ…éœ€è¦ä»¥ä¸‹å¯¹ç­‰ä¾èµ–é¡¹ï¼š

- `@ahoo-wang/fetcher`: æ ¸å¿ƒ HTTP å®¢æˆ·ç«¯åŠŸèƒ½
- `@ahoo-wang/fetcher-decorator`: å£°æ˜å¼ API è£…é¥°å™¨
- `@ahoo-wang/fetcher-eventstream`: æœåŠ¡å™¨å‘é€äº‹ä»¶æ”¯æŒ

ä½¿ç”¨ä¸Šè¿°å‘½ä»¤æ—¶ä¼šè‡ªåŠ¨å®‰è£…è¿™äº›ä¾èµ–ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®¾ç½®

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

// ä½¿ç”¨æ‚¨çš„ API å¯†é’¥åˆå§‹åŒ–å®¢æˆ·ç«¯
const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!, // æ‚¨çš„ OpenAI API å¯†é’¥
});

// åˆ›å»ºç®€å•çš„èŠå¤©è¡¥å…¨
const response = await openai.chat.completions({
  model: 'gpt-3.5-turbo',
  messages: [
    { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚' },
    { role: 'user', content: 'ä½ å¥½ï¼Œä½ æ€ä¹ˆæ ·ï¼Ÿ' },
  ],
  temperature: 0.7,
  max_tokens: 150,
});

console.log(response.choices[0].message.content);
// è¾“å‡º: "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
```

### ç¯å¢ƒå˜é‡è®¾ç½®

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# .env
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
```

åœ¨ä»£ç ä¸­ä½¿ç”¨ï¼š

```typescript
import { config } from 'dotenv';
config(); // åŠ è½½ç¯å¢ƒå˜é‡

const openai = new OpenAI({
  baseURL: process.env.OPENAI_BASE_URL!,
  apiKey: process.env.OPENAI_API_KEY!,
});
```

## ğŸ“¡ æµå¼ç¤ºä¾‹

### åŸºç¡€æµå¼å¤„ç†

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// åˆ›å»ºæµå¼èŠå¤©è¡¥å…¨
const stream = await openai.chat.completions({
  model: 'gpt-4', // ä½¿ç”¨ GPT-4 è·å¾—æ›´å¥½çš„è´¨é‡
  messages: [
    { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªåˆ›æ„è®²æ•…äº‹çš„äººã€‚' },
    { role: 'user', content: 'ç»™æˆ‘è®²ä¸€ä¸ªæœºå™¨äººå­¦ä¹ ç”»ç”»çš„æ•…äº‹' },
  ],
  stream: true,
  temperature: 0.8, // æ›´é«˜çš„åˆ›é€ æ€§
  max_tokens: 1000,
});

// å®æ—¶å¤„ç†æµå¼å“åº”
let fullResponse = '';
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || '';
  if (content) {
    process.stdout.write(content); // å®æ—¶è¾“å‡º
    fullResponse += content;
  }
}

console.log('\n\n--- æµå¼å®Œæˆ ---');
console.log('æ€»å­—ç¬¦æ•°:', fullResponse.length);
```

### é«˜çº§æµå¼å¤„ç†ä¸è¿›åº¦è·Ÿè¸ª

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

const stream = await openai.chat.completions({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: 'å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„ä¿³å¥' }],
  stream: true,
  max_tokens: 1000, // é™åˆ¶ä»¤ç‰Œä»¥è·å¾—æ›´å¿«çš„å“åº”
});

// è·Ÿè¸ªæµå¼è¿›åº¦
let chunksReceived = 0;
let totalContent = '';

for await (const chunk of stream) {
  chunksReceived++;
  const content = chunk.choices[0]?.delta?.content || '';

  if (content) {
    totalContent += content;
    // æ¯ 5 ä¸ªå—æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
    if (chunksReceived % 5 === 0) {
      console.log(
        `å·²æ¥æ”¶ ${chunksReceived} ä¸ªå—ï¼Œ${totalContent.length} ä¸ªå­—ç¬¦`,
      );
    }
  }

  // æ£€æŸ¥å®Œæˆ
  if (chunk.choices[0]?.finish_reason) {
    console.log(`æµå¼å®Œæˆ: ${chunk.choices[0].finish_reason}`);
    break;
  }
}

console.log('æœ€ç»ˆå†…å®¹:', totalContent);
```

## ğŸ“š API å‚è€ƒ

### OpenAI ç±»

æä¾›è®¿é—®æ‰€æœ‰ OpenAI API åŠŸèƒ½çš„ä¸»è¦å®¢æˆ·ç«¯ç±»ã€‚

#### æ„é€ å‡½æ•°

```typescript
new OpenAI(options: OpenAIOptions)
```

ä½¿ç”¨æŒ‡å®šçš„é…ç½®åˆ›å»ºæ–°çš„ OpenAI å®¢æˆ·ç«¯å®ä¾‹ã€‚

**å‚æ•°:**

| å‚æ•°              | ç±»å‹     | å¿…éœ€ | æè¿°                                                        |
| ----------------- | -------- | ---- | ----------------------------------------------------------- |
| `options.baseURL` | `string` | âœ…   | OpenAI API çš„åŸºç¡€ URLï¼ˆä¾‹å¦‚ `'https://api.openai.com/v1'`ï¼‰ |
| `options.apiKey`  | `string` | âœ…   | ç”¨äºèº«ä»½éªŒè¯çš„ OpenAI API å¯†é’¥                              |

**ç¤ºä¾‹:**

```typescript
const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: 'sk-your-api-key-here',
});
```

**æŠ›å‡º:**

- `TypeError`: å¦‚æœæœªæä¾› `apiKey` æˆ– `baseURL`ï¼Œæˆ–å®ƒä»¬ä¸æ˜¯å­—ç¬¦ä¸²

#### å±æ€§

| å±æ€§      | ç±»å‹         | æè¿°                                 |
| --------- | ------------ | ------------------------------------ |
| `fetcher` | `Fetcher`    | é…ç½®äº†èº«ä»½éªŒè¯çš„åº•å±‚ HTTP å®¢æˆ·ç«¯å®ä¾‹ |
| `chat`    | `ChatClient` | ç”¨äºä¸èŠå¤©æ¨¡å‹äº¤äº’çš„èŠå¤©è¡¥å…¨å®¢æˆ·ç«¯   |

### ChatClient

ä¸“é—¨ç”¨äº OpenAI Chat Completions API çš„å®¢æˆ·ç«¯ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”ã€‚

#### æ–¹æ³•

##### `completions<T extends ChatRequest>(chatRequest: T)`

åŸºäºæµå¼é…ç½®åˆ›å»ºå…·æœ‰æ¡ä»¶è¿”å›ç±»å‹çš„èŠå¤©è¡¥å…¨ã€‚

**ç±»å‹å‚æ•°:**

- `T`: æ‰©å±• `ChatRequest` - ç¡®å®šè¿”å›ç±»å‹çš„è¯·æ±‚ç±»å‹

**å‚æ•°:**

- `chatRequest: T` - èŠå¤©è¡¥å…¨è¯·æ±‚é…ç½®

**è¿”å›:**

- å½“ `T['stream']` ä¸º `false` æ—¶è¿”å› `Promise<ChatResponse>`
- å½“ `T['stream']` ä¸º `true` æ—¶è¿”å› `Promise<JsonServerSentEventStream<ChatResponse>>`

**æŠ›å‡º:**

- `Error`: ç½‘ç»œé”™è¯¯ã€èº«ä»½éªŒè¯å¤±è´¥æˆ– API é”™è¯¯
- `EventStreamConvertError`: æ— æ³•å¤„ç†æµå¼å“åº”æ—¶

### æ ¸å¿ƒæ¥å£

#### ChatRequest

èŠå¤©è¡¥å…¨è¯·æ±‚çš„é…ç½®å¯¹è±¡ã€‚

```typescript
interface ChatRequest {
  // æ ¸å¿ƒå‚æ•°
  model?: string; // æ¨¡å‹ IDï¼ˆä¾‹å¦‚ 'gpt-3.5-turbo', 'gpt-4'ï¼‰
  messages: Message[]; // ä¼šè¯æ¶ˆæ¯
  stream?: boolean; // å¯ç”¨æµå¼å“åº”

  // ç”Ÿæˆå‚æ•°
  temperature?: number; // é‡‡æ ·æ¸©åº¦ï¼ˆ0.0 - 2.0ï¼‰
  max_tokens?: number; // ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°
  top_p?: number; // æ ¸é‡‡æ ·å‚æ•°ï¼ˆ0.0 - 1.0ï¼‰

  // æƒ©ç½šå‚æ•°
  frequency_penalty?: number; // é‡å¤æƒ©ç½šï¼ˆ-2.0 - 2.0ï¼‰
  presence_penalty?: number; // ä¸»é¢˜å¤šæ ·æ€§æƒ©ç½šï¼ˆ-2.0 - 2.0ï¼‰

  // é«˜çº§å‚æ•°
  n?: number; // ç”Ÿæˆçš„è¡¥å…¨é€‰æ‹©æ•°é‡
  stop?: string | string[]; // åœæ­¢åºåˆ—
  logit_bias?: Record<string, number>; // ä»¤ç‰Œåå·®è°ƒæ•´
  user?: string; // æœ€ç»ˆç”¨æˆ·æ ‡è¯†ç¬¦

  // å“åº”æ ¼å¼
  response_format?: object; // å“åº”æ ¼å¼è§„èŒƒ

  // å‡½æ•°è°ƒç”¨ï¼ˆæµ‹è¯•ç‰ˆï¼‰
  tools?: any[]; // å¯ç”¨å·¥å…·/å‡½æ•°
  tool_choice?: any; // å·¥å…·é€‰æ‹©ç­–ç•¥

  // å…¶ä»– OpenAI å‚æ•°
  [key: string]: any;
}
```

#### Message

ä¼šè¯ä¸­çš„å•ä¸ªæ¶ˆæ¯è¡¨ç¤ºã€‚

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant' | 'function';
  content: string;
  name?: string; // å‡½æ•°æ¶ˆæ¯çš„åç§°
  function_call?: any; // å‡½æ•°è°ƒç”¨ç»“æœ
}
```

#### ChatResponse

éæµå¼èŠå¤©è¡¥å…¨çš„å“åº”å¯¹è±¡ã€‚

```typescript
interface ChatResponse {
  id: string; // å”¯ä¸€å“åº”æ ‡è¯†ç¬¦
  object: string; // å¯¹è±¡ç±»å‹ï¼ˆé€šå¸¸ä¸º 'chat.completion'ï¼‰
  created: number; // åˆ›å»ºçš„ Unix æ—¶é—´æˆ³
  model: string; // ä½¿ç”¨çš„æ¨¡å‹
  choices: Choice[]; // è¡¥å…¨é€‰æ‹©æ•°ç»„
  usage: Usage; // ä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡
}
```

#### Choice

å•ä¸ªè¡¥å…¨é€‰æ‹©çš„è¡¨ç¤ºã€‚

```typescript
interface Choice {
  index: number; // é€‰æ‹©ç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰
  message: Message; // è¡¥å…¨æ¶ˆæ¯
  finish_reason: string; // è¡¥å…¨åœæ­¢çš„åŸå› 
}
```

#### Usage

è¯·æ±‚çš„ä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡ã€‚

```typescript
interface Usage {
  prompt_tokens: number; // æç¤ºä¸­çš„ä»¤ç‰Œæ•°
  completion_tokens: number; // è¡¥å…¨ä¸­çš„ä»¤ç‰Œæ•°
  total_tokens: number; // ä½¿ç”¨çš„æ€»ä»¤ç‰Œæ•°
}
```

## âš™ï¸ é…ç½®

### ç¯å¢ƒå˜é‡

è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¾¿è½»æ¾é…ç½®ï¼š

```bash
# .env
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
```

åœ¨åº”ç”¨ç¨‹åºä¸­åŠ è½½å®ƒä»¬ï¼š

```typescript
import { config } from 'dotenv';
config();

const openai = new OpenAI({
  baseURL: process.env.OPENAI_BASE_URL!,
  apiKey: process.env.OPENAI_API_KEY!,
});
```

### è‡ªå®šä¹‰åŸºç¡€ URL

ä¸ OpenAI å…¼å®¹çš„ APIã€ä»£ç†æˆ–è‡ªå®šä¹‰éƒ¨ç½²ä¸€èµ·ä½¿ç”¨ï¼š

```typescript
// ä½¿ç”¨ Azure OpenAI
const openai = new OpenAI({
  baseURL:
    'https://your-resource.openai.azure.com/openai/deployments/your-deployment',
  apiKey: 'your-azure-api-key',
});

// ä½¿ç”¨ä»£ç†æœåŠ¡
const openai = new OpenAI({
  baseURL: 'https://your-proxy-service.com/api/openai',
  apiKey: 'your-proxy-api-key',
});

// ä½¿ç”¨æœ¬åœ° OpenAI å…¼å®¹æœåŠ¡å™¨
const openai = new OpenAI({
  baseURL: 'http://localhost:8000/v1',
  apiKey: 'not-needed-for-local',
});
```

### é«˜çº§é…ç½®

#### è‡ªå®šä¹‰ HTTP å®¢æˆ·ç«¯é…ç½®

```typescript
import { Fetcher } from '@ahoo-wang/fetcher';

const customFetcher = new Fetcher({
  baseURL: 'https://api.openai.com/v1',
  headers: {
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
    'Custom-Header': 'value',
    'X-Custom-Client': 'my-app/1.0.0',
  },
  timeout: 30000, // 30 ç§’è¶…æ—¶
  retry: {
    attempts: 3, // é‡è¯•å¤±è´¥çš„è¯·æ±‚
    delay: 1000, // é‡è¯•é—´éš”çš„åˆå§‹å»¶è¿Ÿ
    backoff: 'exponential', // æŒ‡æ•°é€€é¿ç­–ç•¥
  },
  interceptors: [
    {
      request: config => {
        console.log(`Making ${config.method} request to ${config.url}`);
        return config;
      },
      response: response => {
        console.log(`Response: ${response.status}`);
        return response;
      },
    },
  ],
});

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// æ›¿æ¢é»˜è®¤çš„ fetcher
openai.fetcher = customFetcher;
```

#### è¯·æ±‚æ‹¦æˆªå™¨

```typescript
const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// æ·»åŠ æ—¥å¿—è®°å½•æ‹¦æˆªå™¨
openai.fetcher.interceptors.request.use(config => {
  console.log('Request:', {
    url: config.url,
    method: config.method,
    headers: config.headers,
    body: config.body,
  });
  return config;
});

openai.fetcher.interceptors.response.use(response => {
  console.log('Response:', {
    status: response.status,
    headers: response.headers,
    data: response.data,
  });
  return response;
});
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†

åº“æä¾›å…¨é¢çš„é”™è¯¯å¤„ç†ï¼Œå…·æœ‰è¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯å’Œæ­£ç¡®çš„é”™è¯¯ç±»å‹ã€‚

### åŸºç¡€é”™è¯¯å¤„ç†

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

try {
  const response = await openai.chat.completions({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼Œä¸–ç•Œï¼' }],
  });

  console.log('æˆåŠŸ:', response.choices[0].message.content);
} catch (error) {
  console.error('OpenAI API é”™è¯¯:', error.message);
  console.error('é”™è¯¯è¯¦æƒ…:', error);
}
```

### é«˜çº§é”™è¯¯å¤„ç†ä¸çŠ¶æ€ç 

```typescript
try {
  const response = await openai.chat.completions({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼' }],
  });
} catch (error: any) {
  // å¤„ç†ä¸åŒé”™è¯¯ç±»å‹
  if (error.response) {
    // API è¿”å›äº†é”™è¯¯å“åº”
    const status = error.response.status;
    const data = error.response.data;

    switch (status) {
      case 401:
        console.error('èº«ä»½éªŒè¯å¤±è´¥ - æ£€æŸ¥æ‚¨çš„ API å¯†é’¥');
        break;
      case 429:
        console.error('é€Ÿç‡é™åˆ¶è¶…è¿‡ - å®æ–½é€€é¿ç­–ç•¥');
        console.log('é‡è¯•é—´éš”:', data.retry_after, 'ç§’');
        break;
      case 400:
        console.error('è¯·æ±‚é”™è¯¯ - æ£€æŸ¥æ‚¨çš„å‚æ•°');
        console.log('é”™è¯¯è¯¦æƒ…:', data.error);
        break;
      case 500:
      case 502:
      case 503:
        console.error('OpenAI æœåŠ¡å™¨é”™è¯¯ - ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•');
        break;
      default:
        console.error(`æ„å¤–é”™è¯¯ (${status}):`, data.error?.message);
    }
  } else if (error.request) {
    // ç½‘ç»œé”™è¯¯
    console.error('ç½‘ç»œé”™è¯¯ - æ£€æŸ¥æ‚¨çš„äº’è”ç½‘è¿æ¥');
    console.error('è¯·æ±‚è¯¦æƒ…:', error.request);
  } else {
    // å…¶ä»–é”™è¯¯
    console.error('æ„å¤–é”™è¯¯:', error.message);
  }
}
```

### æµå¼é”™è¯¯å¤„ç†

```typescript
try {
  const stream = await openai.chat.completions({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ç»™æˆ‘è®²ä¸ªæ•…äº‹' }],
    stream: true,
  });

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content;
    if (content) {
      process.stdout.write(content);
    }
  }
} catch (error) {
  if (error.name === 'EventStreamConvertError') {
    console.error('æµå¼é”™è¯¯ - å“åº”å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„æœåŠ¡å™¨å‘é€äº‹ä»¶æµ');
  } else {
    console.error('æµå¼å¤±è´¥:', error.message);
  }
}
```

### é‡è¯•é€»è¾‘å®ç°

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

class ResilientOpenAI {
  private client: OpenAI;
  private maxRetries: number;
  private baseDelay: number;

  constructor(apiKey: string, maxRetries = 3, baseDelay = 1000) {
    this.client = new OpenAI({
      baseURL: 'https://api.openai.com/v1',
      apiKey,
    });
    this.maxRetries = maxRetries;
    this.baseDelay = baseDelay;
  }

  private async delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  async completions(request: any, attempt = 1): Promise<any> {
    try {
      return await this.client.chat.completions(request);
    } catch (error: any) {
      const isRetryable =
        error.response?.status >= 500 ||
        error.response?.status === 429 ||
        !error.response; // ç½‘ç»œé”™è¯¯

      if (isRetryable && attempt <= this.maxRetries) {
        const delay = this.baseDelay * Math.pow(2, attempt - 1); // æŒ‡æ•°é€€é¿
        console.log(`å°è¯• ${attempt} å¤±è´¥ï¼Œ${delay}ms åé‡è¯•...`);
        await this.delay(delay);
        return this.completions(request, attempt + 1);
      }

      throw error;
    }
  }
}

// ä½¿ç”¨
const resilientClient = new ResilientOpenAI(process.env.OPENAI_API_KEY!);

try {
  const response = await resilientClient.completions({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼' }],
  });
  console.log(response.choices[0].message.content);
} catch (error) {
  console.error('æ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥:', error.message);
}
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ Fetcher é…ç½®

```typescript
import { Fetcher } from '@ahoo-wang/fetcher';

const customFetcher = new Fetcher({
  baseURL: 'https://api.openai.com/v1',
  headers: {
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
    'Custom-Header': 'value',
    'X-Custom-Client': 'my-app/1.0.0',
  },
  timeout: 30000, // 30 ç§’è¶…æ—¶
  retry: {
    attempts: 3, // é‡è¯•å¤±è´¥çš„è¯·æ±‚
    delay: 1000, // é‡è¯•é—´éš”çš„åˆå§‹å»¶è¿Ÿ
    backoff: 'exponential', // æŒ‡æ•°é€€é¿ç­–ç•¥
  },
  interceptors: [
    {
      request: config => {
        console.log(`Making ${config.method} request to ${config.url}`);
        return config;
      },
      response: response => {
        console.log(`Response: ${response.status}`);
        return response;
      },
    },
  ],
});

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// æ›¿æ¢é»˜è®¤çš„ fetcher
openai.fetcher = customFetcher;
```

### å‡½æ•°è°ƒç”¨ï¼ˆæµ‹è¯•ç‰ˆï¼‰

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// å®šä¹‰å¯ç”¨å‡½æ•°
const functions = [
  {
    name: 'get_weather',
    description: 'è·å–æŸä¸ªä½ç½®çš„å½“å‰å¤©æ°”',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚ï¼šæ—§é‡‘å±±ï¼ŒåŠ åˆ©ç¦å°¼äºš',
        },
      },
      required: ['location'],
    },
  },
];

// ä½¿ç”¨å‡½æ•°è°ƒç”¨è¿›è¡Œè¯·æ±‚
const response = await openai.chat.completions({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'æ—§é‡‘å±±çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ' }],
  functions: functions,
  function_call: 'auto', // è®©æ¨¡å‹å†³å®šä½•æ—¶è°ƒç”¨å‡½æ•°
});

// å¤„ç†å‡½æ•°è°ƒç”¨
if (response.choices[0].message.function_call) {
  const functionCall = response.choices[0].message.function_call;
  console.log('è°ƒç”¨çš„å‡½æ•°:', functionCall.name);
  console.log('å‚æ•°:', JSON.parse(functionCall.arguments));
}
```

### ä¼šè¯ç®¡ç†

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

class ChatConversation {
  private client: OpenAI;
  private messages: Array<{ role: string; content: string }> = [];

  constructor(apiKey: string) {
    this.client = new OpenAI({
      baseURL: 'https://api.openai.com/v1',
      apiKey,
    });
  }

  async addMessage(role: 'system' | 'user' | 'assistant', content: string) {
    this.messages.push({ role, content });
  }

  async sendMessage(content: string, options: Partial<ChatRequest> = {}) {
    await this.addMessage('user', content);

    const response = await this.client.chat.completions({
      model: 'gpt-3.5-turbo',
      messages: this.messages,
      ...options,
    });

    const assistantMessage = response.choices[0].message;
    await this.addMessage('assistant', assistantMessage.content);

    return assistantMessage;
  }

  getHistory() {
    return [...this.messages];
  }

  clearHistory() {
    this.messages = [];
  }
}

// ä½¿ç”¨
const conversation = new ChatConversation(process.env.OPENAI_API_KEY!);

// è®¾ç½®ç³»ç»Ÿæç¤º
await conversation.addMessage('system', 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ç¼–ç¨‹åŠ©æ‰‹ã€‚');

// è¿›è¡Œå¯¹è¯
const response1 = await conversation.sendMessage('å¦‚ä½•ä½¿ç”¨ TypeScriptï¼Ÿ');
console.log('åŠ©æ‰‹:', response1.content);

const response2 = await conversation.sendMessage('èƒ½ç»™æˆ‘ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ');
console.log('åŠ©æ‰‹:', response2.content);
```

### æ‰¹é‡å¤„ç†

```typescript
import { OpenAI } from '@ahoo-wang/fetcher-openai';

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

async function processBatch(prompts: string[], batchSize = 5) {
  const results = [];

  // åˆ†æ‰¹å¤„ç†ä»¥é¿å…é€Ÿç‡é™åˆ¶
  for (let i = 0; i < prompts.length; i += batchSize) {
    const batch = prompts.slice(i, i + batchSize);

    const batchPromises = batch.map(prompt =>
      openai.chat.completions({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.7,
      }),
    );

    try {
      const batchResults = await Promise.all(batchPromises);
      results.push(...batchResults);

      // åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿä»¥éµå®ˆé€Ÿç‡é™åˆ¶
      if (i + batchSize < prompts.length) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    } catch (error) {
      console.error(`æ‰¹æ¬¡ ${Math.floor(i / batchSize) + 1} å¤±è´¥:`, error);
      // ç»§ç»­ä¸‹ä¸€ä¸ªæ‰¹æ¬¡æˆ–å®æ–½é‡è¯•é€»è¾‘
    }
  }

  return results;
}

// ä½¿ç”¨
const prompts = [
  'è§£é‡Šé‡å­è®¡ç®—',
  'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
  'åŒºå—é“¾å¦‚ä½•å·¥ä½œï¼Ÿ',
  'æè¿°äº‘è®¡ç®—',
];

const results = await processBatch(prompts);
results.forEach((result, index) => {
  console.log(`æç¤º ${index + 1}:`, result.choices[0].message.content);
});
```

### ä¸å…¶ä»– Fetcher åŠŸèƒ½é›†æˆ

ç”±äºæ­¤åº“åŸºäº Fetcher ç”Ÿæ€æ„å»ºï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æ‰€æœ‰ Fetcher åŠŸèƒ½ï¼š

#### è¯·æ±‚/å“åº”æ‹¦æˆªå™¨

```typescript
// æ·»åŠ è¯·æ±‚æ—¥å¿—è®°å½•
openai.fetcher.interceptors.request.use(config => {
  console.log(`[${new Date().toISOString()}] ${config.method} ${config.url}`);
  return config;
});

openai.fetcher.interceptors.response.use(response => {
  console.log(`[${new Date().toISOString()}] Response: ${response.status}`);
  return response;
});
```

#### è‡ªå®šä¹‰ç»“æœæå–å™¨

```typescript
import { ResultExtractor } from '@ahoo-wang/fetcher';

// åˆ›å»ºæ·»åŠ å…ƒæ•°æ®çš„è‡ªå®šä¹‰æå–å™¨
const metadataExtractor: ResultExtractor = exchange => {
  const response = exchange.response;
  return {
    ...response,
    _metadata: {
      requestId: response.headers.get('x-request-id'),
      processingTime: Date.now() - exchange.startTime,
      model: response.model,
    },
  };
};

// ä¸èŠå¤©è¡¥å…¨ä¸€èµ·ä½¿ç”¨
const response = await openai.chat.completions(
  {
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼' }],
  },
  {
    resultExtractor: metadataExtractor,
  },
);
```

#### è¯·æ±‚å»é‡

```typescript
// ä¸ºç›¸åŒçš„è¯·æ±‚å¯ç”¨è¯·æ±‚å»é‡
openai.fetcher.defaults.deduplicate = true;

// è¿™å°†é‡ç”¨ç›¸åŒå¹¶å‘è¯·æ±‚çš„å“åº”
const [response1, response2] = await Promise.all([
  openai.chat.completions({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼' }],
  }),
  openai.chat.completions({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼' }],
  }),
]);
```

## ğŸ”„ è¿ç§»æŒ‡å—

### ä» OpenAI SDK è¿ç§»

å¦‚æœæ‚¨è¦ä»å®˜æ–¹ OpenAI SDK è¿ç§»ï¼š

```typescript
// ä¹‹å‰ (OpenAI SDK)
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const response = await openai.chat.completions.create({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: 'ä½ å¥½!' }],
});

// ä¹‹å (Fetcher OpenAI)
import { OpenAI } from '@ahoo-wang/fetcher-openai';

const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

const response = await openai.chat.completions({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: 'ä½ å¥½!' }],
});
```

### ä¸»è¦å·®å¼‚

| åŠŸèƒ½           | OpenAI SDK            | Fetcher OpenAI            |
| -------------- | --------------------- | ------------------------- |
| **æµå¼**       | `for await (...)`     | `for await (...)` (ç›¸åŒ)  |
| **é”™è¯¯å¤„ç†**   | è‡ªå®šä¹‰é”™è¯¯ç±»å‹        | æ ‡å‡† JavaScript é”™è¯¯      |
| **é…ç½®**       | `new OpenAI(options)` | `new OpenAI(options)`     |
| **TypeScript** | å®Œæ•´æ”¯æŒ              | å®Œæ•´æ”¯æŒå’Œæ¡ä»¶ç±»å‹        |
| **æ‹¦æˆªå™¨**     | æœ‰é™                  | å®Œæ•´çš„ Fetcher æ‹¦æˆªå™¨æ”¯æŒ |
| **åŒ…ä½“ç§¯**     | è¾ƒå¤§                  | ç»è¿‡æ ‘æ‘‡ä¼˜åŒ–çš„ä½“ç§¯        |

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### èº«ä»½éªŒè¯é”™è¯¯

**é—®é¢˜ï¼š** `401 Unauthorized` é”™è¯¯

**è§£å†³æ–¹æ¡ˆï¼š**

```typescript
// 1. æ£€æŸ¥æ‚¨çš„ API å¯†é’¥
const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!, // ç¡®ä¿å·²è®¾ç½®
});

// 2. éªŒè¯ API å¯†é’¥æ ¼å¼
if (!process.env.OPENAI_API_KEY?.startsWith('sk-')) {
  throw new Error('API å¯†é’¥æ ¼å¼æ— æ•ˆ');
}
```

#### æµå¼å¤„ç†ä¸å·¥ä½œ

**é—®é¢˜ï¼š** æµå¼å“åº”æœªæŒ‰é¢„æœŸå·¥ä½œ

**è§£å†³æ–¹æ¡ˆï¼š**

```typescript
// 1. ç¡®ä¿ stream å‚æ•°è®¾ç½®ä¸º true
const stream = await openai.chat.completions({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: 'ä½ å¥½!' }],
  stream: true, // è¿™æ˜¯å¿…éœ€çš„
});

// 2. æ­£ç¡®å¤„ç†æµ
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content); // ä½¿ç”¨ process.stdout.write è¿›è¡Œå®æ—¶è¾“å‡º
  }
}
```

#### é€Ÿç‡é™åˆ¶

**é—®é¢˜ï¼š** `429 Too Many Requests` é”™è¯¯

**è§£å†³æ–¹æ¡ˆï¼š**

```typescript
// å®æ–½æŒ‡æ•°é€€é¿
async function completionsWithRetry(request: any, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await openai.chat.completions(request);
    } catch (error: any) {
      if (error.response?.status === 429 && attempt < maxRetries) {
        const delay = Math.pow(2, attempt) * 1000; // æŒ‡æ•°é€€é¿
        await new Promise(resolve => setTimeout(resolve, delay));
        continue;
      }
      throw error;
    }
  }
}
```

#### ç½‘ç»œé”™è¯¯

**é—®é¢˜ï¼š** è¿æ¥è¶…æ—¶æˆ–ç½‘ç»œæ•…éšœ

**è§£å†³æ–¹æ¡ˆï¼š**

```typescript
// é…ç½®è¶…æ—¶å’Œé‡è¯•
const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// è®¾ç½®è‡ªå®šä¹‰è¶…æ—¶
openai.fetcher.defaults.timeout = 60000; // 60 ç§’

// æ·»åŠ é‡è¯•é€»è¾‘
openai.fetcher.defaults.retry = {
  attempts: 3,
  delay: 1000,
  backoff: 'exponential',
};
```

### è°ƒè¯•æ¨¡å¼

å¯ç”¨è°ƒè¯•æ—¥å¿—è®°å½•ä»¥æ’é™¤é—®é¢˜ï¼š

```typescript
// å¯ç”¨è¯·æ±‚æ—¥å¿—è®°å½•
openai.fetcher.interceptors.request.use(config => {
  console.log('Request:', {
    url: config.url,
    method: config.method,
    headers: config.headers,
    body: config.body,
  });
  return config;
});

openai.fetcher.interceptors.response.use(response => {
  console.log('Response:', {
    status: response.status,
    headers: response.headers,
    data: response.data,
  });
  return response;
});
```

## âš¡ æ€§èƒ½æç¤º

### ä¼˜åŒ–åŒ…ä½“ç§¯

```typescript
// ä»…å¯¼å…¥æ‚¨éœ€è¦çš„åŠŸèƒ½
import { OpenAI } from '@ahoo-wang/fetcher-openai';

// é¿å…å¯¼å…¥æœªä½¿ç”¨çš„åŠŸèƒ½
// âŒ ä¸è¦è¿™æ ·åšï¼Œå¦‚æœæ‚¨åªéœ€è¦èŠå¤©è¡¥å…¨
// import * as OpenAI from '@ahoo-wang/fetcher-openai';
```

### è¿æ¥æ± åŒ–

å¯¹äºé«˜ååé‡åº”ç”¨ç¨‹åºï¼š

```typescript
// ä½¿ç”¨ HTTP/2 å…¼å®¹å®¢æˆ·ç«¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
const openai = new OpenAI({
  baseURL: 'https://api.openai.com/v1',
  apiKey: process.env.OPENAI_API_KEY!,
});

// ä¸º Node.js é…ç½®è¿æ¥æ± åŒ–
process.env.NODE_OPTIONS = '--max-http-header-size=81920';
```

### æµå¼ä¼˜åŒ–

```typescript
// é«˜æ•ˆå¤„ç†æµå¼å“åº”
const stream = await openai.chat.completions({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: 'è¿™é‡Œæ˜¯é•¿å“åº”...' }],
  stream: true,
  max_tokens: 1000, // é™åˆ¶ä»¤ç‰Œä»¥è·å¾—æ›´å¿«çš„å“åº”
});

// ä½¿ç”¨é«˜æ•ˆçš„æµå¼å¤„ç†
let buffer = '';
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || '';
  buffer += content;

  // æŒ‰å—å¤„ç†è€Œä¸æ˜¯é€å­—ç¬¦
  if (buffer.length >= 100) {
    processChunk(buffer);
    buffer = '';
  }
}
```

### ç¼“å­˜ç­–ç•¥

```typescript
// ä¸ºç›¸ä¼¼è¯·æ±‚å®æ–½å“åº”ç¼“å­˜
class CachedOpenAI {
  private cache = new Map<string, any>();
  private client: OpenAI;

  constructor(apiKey: string) {
    this.client = new OpenAI({
      baseURL: 'https://api.openai.com/v1',
      apiKey,
    });
  }

  private getCacheKey(request: any): string {
    return JSON.stringify({
      model: request.model,
      messages: request.messages,
      temperature: request.temperature,
    });
  }

  async completions(request: any, useCache = true) {
    const cacheKey = this.getCacheKey(request);

    if (useCache && this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey);
    }

    const response = await this.client.chat.completions(request);

    if (useCache) {
      this.cache.set(cacheKey, response);
    }

    return response;
  }
}
```

## ğŸ¤ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](../../CONTRIBUTING.md)äº†è§£è¯¦æƒ…ã€‚

### å¼€å‘è®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/Ahoo-Wang/fetcher.git
cd fetcher

# å®‰è£…ä¾èµ–
pnpm install

# ä¸ºæ­¤åŒ…è¿è¡Œæµ‹è¯•
pnpm --filter @ahoo-wang/fetcher-openai test

# æ„å»ºåŒ…
pnpm --filter @ahoo-wang/fetcher-openai build
```

### ä»£ç é£æ ¼

æ­¤é¡¹ç›®ä½¿ç”¨ ESLint å’Œ Prettier è¿›è¡Œä»£ç æ ¼å¼åŒ–ã€‚è¯·ç¡®ä¿æ‚¨çš„ä»£ç éµå¾ªæ—¢å®šçš„æ¨¡å¼ï¼š

```bash
# ä»£ç æ£€æŸ¥
pnpm --filter @ahoo-wang/fetcher-openai lint

# æ ¼å¼åŒ–ä»£ç 
pnpm format
```

## ğŸ“„ è®¸å¯è¯

æ ¹æ® Apache License, Version 2.0 è®¸å¯è¯æˆæƒã€‚è¯¦è§ [LICENSE](../../LICENSE)ã€‚

## ğŸ”— ç›¸å…³åŒ…

- [**@ahoo-wang/fetcher**](https://github.com/Ahoo-Wang/fetcher/tree/master/packages/fetcher) - å…·æœ‰é«˜çº§åŠŸèƒ½çš„æ ¸å¿ƒ HTTP å®¢æˆ·ç«¯
- [**@ahoo-wang/fetcher-decorator**](https://github.com/Ahoo-Wang/fetcher/tree/master/packages/fetcher-decorator) - ç”¨äºç±»å‹å®‰å…¨è¯·æ±‚çš„å£°æ˜å¼ API è£…é¥°å™¨
- [**@ahoo-wang/fetcher-eventstream**](https://github.com/Ahoo-Wang/fetcher/tree/master/packages/fetcher-eventstream) - å®æ—¶æµå¼å¤„ç†çš„æœåŠ¡å™¨å‘é€äº‹ä»¶æ”¯æŒ
- [**@ahoo-wang/fetcher-openapi**](https://github.com/Ahoo-Wang/fetcher/tree/master/packages/openapi) - OpenAPI è§„èŒƒå®¢æˆ·ç«¯ç”Ÿæˆ

---

<p align="center">
  <strong>ä½¿ç”¨ Fetcher ç”Ÿæ€ â¤ï¸ æ„å»º</strong>
</p>

<p align="center">
  <a href="https://github.com/Ahoo-Wang/fetcher">GitHub</a> â€¢
  <a href="https://www.npmjs.com/package/@ahoo-wang/fetcher-openai">NPM</a> â€¢
  <a href="https://deepwiki.com/Ahoo-Wang/fetcher">æ–‡æ¡£</a>
</p>
